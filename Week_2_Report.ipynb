{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "- Is used whenever we want to predict a certain outcome from a given input, and we have examples of output/output pairs.\n",
    "- We build machine learning model from these input/output pairs which we comprise our training set.\n",
    "- Our goal is to make accurate predictions for new, never-before-seen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Supervised Machine Learning problems\n",
    "1. Classification\n",
    "2. Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "The goal is to predict a class label, which is a choice from a predifined list of posiblities.\n",
    "1. **Binary Classification:** distinguish between exactly two classes (ie. 0 and 1).\n",
    "2. **Multiclass Classification:** classification between more than two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "The goal is to predict a continous number, or floating point number in programming terms (or real number in mathematical terms).\n",
    "- For example predicting a person's annual income from their education, their age, and where they live is an example of a regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization, Overfitting and Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization\n",
    "If a model is able to make accurate on unseen data, we say it is able to ***generalize*** from training set to the test set. So we want to build a model that is able generalize as accurately as possible.\n",
    "### Overfitting\n",
    "Building a model that is too complex for the amount of information we have, is called ***Overfitting***.\n",
    "Occurs when you fit a model too closely to the particularities of the training set and obtain a model that works well on the training set but is not able to generalize to new data.\n",
    "### Underfitting\n",
    "If your model is too simple then you might not be able to capture all the aspects of and variability in the data, and your model will do badly even on the training set. So choosing to simple a model is called ***Underfitting***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors\n",
    "Is arguably the simplest machine learning algorithm. Building the model consists only of storing the training dataset. To make a prediction for a new data point, the algorithm finds the closest data points in the training dataset (its \"nearest neighbors\").\n",
    "- **K-Neighbors Classification:**\n",
    "In its simplest version, the K-NN algorithm only considers exactly one nearest neighbor, which is the closest training data point to the point we want to make a prediction for.\n",
    "### Important parameters to the K-Neighbors Classifier\n",
    "  1. The number of neighbors\n",
    "  2. How you can measure distance between data points.\n",
    "   - Euclidean distance is used, which works well in many settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Models\n",
    "Are a class of models that are widely used in practice and have been studied extensively in the last few decades, with roots going back over a hundred years.\n",
    "Linear models make a prediction using a ***linear function*** of the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear models for Regression\n",
    "Can be characterized as regression models for which the prediction is a line for a single feature, a plane when using two features, or a hper-plane in higher dimensions (that is, when using more features).\n",
    "- There are many different linear models for regression. The difference between these models lies in how the model parameters ***W*** and ***b*** are learned from the training data, and how model complexity can be controlled.\n",
    "- **Linear Regression:**\n",
    "Is the simplest and most classic linear method for regression. Linear regression finds the parameters ***W*** and ***b*** that minimize the *mean square error* between predictions and the true regression target on the training set.\n",
    "- **Ridge Regression:**\n",
    "In ridge regression the coefficients ***W*** are chosen not only so that they predict well on the training data, but also to fit an additional constraint. We also want the magnitude of coefficients to be small as possible. In other words, all entries of ***W*** should be close to zero (**L2 regularization**).\n",
    "- **Lasso:**\n",
    "An alternative to Ridge for regularizing linear regression is ***Lasso***. Also is used to restricts coefficients to be close zero, but in a slightly different way called ***L1 regularization***.\n",
    "\n",
    "#### Linear models for Classification\n",
    "\n",
    "The decision boundary is a linear function of the input.\n",
    "- **Binary linear classifier:** Is a classifier that separates two classes using a line, a plane, or a hyperplane.\n",
    "- The two most common linear classification algorithms are ***Logistic regression*** and ***Linear support vector machines***.\n",
    "- The two models come up with similar decision boundaries. Both models apply L2 regularization, in the same way that Ridge does for regression.\n",
    "\n",
    "#### Linear models for multiclass Classification\n",
    "\n",
    "##### Decision Trees\n",
    "Are widely used models for classification and regression tasks. They learn a hierrachy of ***if/else*** questions leading a decision.\n",
    "##### Ensemble of Decision Trees\n",
    "- **Ensemble:** Are methods that combine multiple machine learning models to create more powerful models.\n",
    "\n",
    "##### Random Forests\n",
    "A main drawback of decision trees is that they tend to overfit the training data. **Random forests** are one way to address this problem. A random forests is essentially a collection of decision trees, when each tree is slightly different from others.\n",
    "The idea behind random forests is that each tree might do a relatively good job of predicting, but will likely overfit on part of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
